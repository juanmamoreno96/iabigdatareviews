{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46e99c4-8ca1-47df-8660-9d7f47071739",
   "metadata": {},
   "source": [
    "# Detección de reseñas positivas y negativas usando BERT - Trabajo final\n",
    "\n",
    "En este proyecto, vamos a aplicar técnicas avanzadas de procesamiento del lenguaje natural (NLP) utilizando BERT (Bidirectional Encoder Representations from Transformers) para analizar y clasificar reseñas de clientes de una tienda de ropa como positivas o negativas. Utilizaremos un conjunto de datos que contiene reseñas etiquetadas previamente para entrenar y evaluar nuestro modelo.\n",
    "\n",
    "<center><img src=\"../img/fashion_boutique.webp\" alt=\"\" title=\"FashionBoutique\" width=\"150\" /></center>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78ea17-77c7-4550-bb07-a0061673c4fd",
   "metadata": {},
   "source": [
    "## 1. Problema a resolver: \n",
    "\n",
    "**Problema:** _Reducir las devoluciones de una tienda de ropa local, para poder yo como dueño de la tienda ver cuales son las causas por los que el usuario devuelve la prenda. Así poder mejorar las caracteristicas de las prendas o qué tipo de ropa quieren los compradores que tenga en dicha tienda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a92f78f2-7e16-479b-9594-97d57915d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# esto es para extraer los datos de ese archivo .tgz\n",
    "import tarfile\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90b9bb6b-8e2d-4f0d-8a80-bf9e99452d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JuanManuelMorenoRuiz\\Desktop\\Cursos\\BOOTCAMP IA + BIG DATA\\iabigdatareviews\\notebooks\n",
      "C:\\Users\\JuanManuelMorenoRuiz\\Desktop\\Cursos\\BOOTCAMP IA + BIG DATA\\iabigdatareviews\\notebooks\\..\\data\\raw\\balanced_review.csv\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset original\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "file_path = os.path.join(current_dir, '..', 'data', 'raw', 'balanced_review.csv')\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb96a03c-99c3-45df-829f-0ae3f3262074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText  \\\n",
      "0        5  I'm enjoying these, the fit is about right and...   \n",
      "1        5  these are the coolest little board shorts.  Th...   \n",
      "2        5  beautiful ring for the money, looks very expen...   \n",
      "3        5  I love this dress! Fantastic quality, great st...   \n",
      "4        5  These Ecco golf shoes are differently shaped t...   \n",
      "\n",
      "              summary  \n",
      "0    Very comfortable  \n",
      "1   funky board short  \n",
      "2          Five Stars  \n",
      "3   I love this dress  \n",
      "4  Still A Great Fit!  \n",
      "792000\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset original\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Visualizar las primeras filas para entender la estructura\n",
    "print(df.head())\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e2087c8-cfae-444d-aa91-323f0e34ab66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText\n",
      "0        5  I'm enjoying these, the fit is about right and...\n",
      "1        5  these are the coolest little board shorts.  Th...\n",
      "2        5  beautiful ring for the money, looks very expen...\n",
      "3        5  I love this dress! Fantastic quality, great st...\n",
      "4        5  These Ecco golf shoes are differently shaped t...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "# Eliminamos la columna 'summary'\n",
    "df = df[['overall', 'reviewText']]\n",
    "\n",
    "# Verificamos la estructura actualizada del dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87d58a03-e9b5-4251-8a07-4c55210fdbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JuanManuelMorenoRuiz\\Desktop\\Cursos\\BOOTCAMP IA + BIG DATA\\iabigdatareviews\\notebooks\\..\\data\\modified\\balanced_review_modified.csv\n"
     ]
    }
   ],
   "source": [
    "# Guardamos el dataset modificado en un nuevo archivo CSV\n",
    "\n",
    "output_dir = os.path.join(current_dir, '..', 'data', 'modified' ,'balanced_review_modified.csv')\n",
    "\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f7456331-aae5-4a4c-bf3a-456b51d719ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_dir, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9abf2a75-c2ca-4e86-abec-e7074aab5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_1_df = df.loc[df['overall'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1d3d956-ab3f-4ec9-9244-b1bd39b1ae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        overall                                         reviewText\n",
      "311478        1  I'm a true size small...reading 2 other review...\n",
      "261836        1  Not what I expected\\nReally bad\\nUncomfortable...\n",
      "68177         1               Small and the crotch ripped quickly.\n",
      "453575        1  Awesome coat and awesome price. Very warm and ...\n",
      "45291         1                   It ripped within two days of use\n"
     ]
    }
   ],
   "source": [
    "print(overall_1_df.sample(5))  # Muestra 5 registros aleatorios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "681040b6-81e9-49d1-8af7-1b034558bff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        overall                                         reviewText\n",
      "0             5  I'm enjoying these, the fit is about right and...\n",
      "1             5  these are the coolest little board shorts.  Th...\n",
      "2             5  beautiful ring for the money, looks very expen...\n",
      "3             5  I love this dress! Fantastic quality, great st...\n",
      "4             5  These Ecco golf shoes are differently shaped t...\n",
      "...         ...                                                ...\n",
      "791995        1  Its just like every other shaper uncomfortable...\n",
      "791996        1  The fit is fine and they're comfortable. Howev...\n",
      "791997        1             correct size but pops open very easily\n",
      "791998        1  Only rated this with one star because you didn...\n",
      "791999        1   The product came without the tags on the side!!!\n",
      "\n",
      "[792000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cf46e769-3e4d-4011-8e78-a9b9dab8cc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "792000\n"
     ]
    }
   ],
   "source": [
    "num_filas = df.shape[0]\n",
    "print(num_filas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d58261a5-3129-443f-bd6f-bef711df5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JuanManuelMorenoRuiz\\Desktop\\Cursos\\BOOTCAMP IA + BIG DATA\\iabigdatareviews\\notebooks\\..\\data\\modified\\balanced_review_modified_binario.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(current_dir, '..', 'data', 'modified' ,'balanced_review_modified_binario.csv')\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ef273a9-3024-45d6-a140-23b383ab4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(output_dir)\n",
    "#output_dir = os.path.join(current_dir, '..', 'data', 'modified' ,'balanced_review_modified_no_header.csv')\n",
    "#Exportar sin cabeceras\n",
    "#df.to_csv(output_dir, index=False, header=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "613e9aa5-1184-4036-9b60-dfc7e66d0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e43825ae-62c8-4820-af5d-bc30e5a83b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0edb6eb2-3dba-46b8-a7b6-1d8a33409ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 633600\n",
      "Tamaño del conjunto de prueba: 158400\n"
     ]
    }
   ],
   "source": [
    "print(f'Tamaño del conjunto de entrenamiento: {len(train_df)}')\n",
    "print(f'Tamaño del conjunto de prueba: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "97f6a2b7-84c0-4a64-9972-82c78db2d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_train = os.path.join(current_dir, '..', 'data', 'modified' ,'train.csv')\n",
    "output_dir_test = os.path.join(current_dir, '..', 'data', 'modified' ,'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "96414585-7571-47e2-9d4d-dd5d31be1a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los conjuntos de entrenamiento y prueba han sido guardados como train.csv y test.csv\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(output_dir_train, index=False)\n",
    "test_df.to_csv(output_dir_test, index=False)\n",
    "\n",
    "print('Los conjuntos de entrenamiento y prueba han sido guardados como train.csv y test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8e691a13-8e07-4aab-815f-4b13bd0c1dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de elementos no numéricos en la columna 0: 0\n"
     ]
    }
   ],
   "source": [
    "column_0 = train_df.iloc[:, 0]  # Obtener la columna 0 como una Serie\n",
    "numeric_values = pd.to_numeric(column_0, errors='coerce')\n",
    "non_numeric_count = numeric_values.isna().sum()\n",
    "print(f\"Número de elementos no numéricos en la columna 0: {non_numeric_count}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "89864709-2fd4-4561-a042-0f9604ee014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText\n",
      "0        2  It is a beautiful top! Very soft and very comf...\n",
      "1        1  It's much lighter than the other New Balance r...\n",
      "2        1  I can only use this purse when I have very lit...\n",
      "3        1  I like the style and comfort of the watch.  Th...\n",
      "4        1  Lost my Driver License when the back pocket un...\n"
     ]
    }
   ],
   "source": [
    "#train_df = pd.read_csv(output_dir_train)\n",
    "train_df = pd.read_csv(output_dir_train, header=0, names=['overall', 'reviewText'])\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b2298c1c-f491-4686-be7b-a7223d3a789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de elementos no numéricos en la columna 0: 0\n"
     ]
    }
   ],
   "source": [
    "column_0 = train_df.iloc[:, 0]  # Obtener la columna 0 como una Serie\n",
    "numeric_values = pd.to_numeric(column_0, errors='coerce')\n",
    "non_numeric_count = numeric_values.isna().sum()\n",
    "print(f\"Número de elementos no numéricos en la columna 0: {non_numeric_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9068cb30-7153-49d7-bad6-8a8eb57d3a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de valores nulos en la columna 0: 0\n"
     ]
    }
   ],
   "source": [
    "null_count = train_df.iloc[:, 0].isnull().sum()\n",
    "print(f\"Número de valores nulos en la columna 0: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "318c4ed5-f670-4e71-bc5f-c921c77c6e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de elementos no numéricos en la columna 0: 0\n",
      "Número de valores nulos en la columna 0: 0\n"
     ]
    }
   ],
   "source": [
    "numeric_values = pd.to_numeric(train_df.iloc[:, 0], errors='coerce')\n",
    "non_numeric_count = numeric_values.isna().sum()\n",
    "\n",
    "# Verificar valores nulos\n",
    "null_count = train_df.iloc[:, 0].isnull().sum()\n",
    "\n",
    "print(f\"Número de elementos no numéricos en la columna 0: {non_numeric_count}\")\n",
    "print(f\"Número de valores nulos en la columna 0: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a147856d-e38a-42be-9cbb-67a63f00b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText\n",
      "0        2  My son was so happy with these shoes. I only g...\n",
      "1        1  I bought a large and it does not quite do the ...\n",
      "2        1  Color is not as advertised. It's way darker in...\n",
      "3        1        Its definitely a tote sized bag not a purse\n",
      "4        1  I'm 5'10 140 lbs. I am no means a medium or a ...\n"
     ]
    }
   ],
   "source": [
    "#test_df = pd.read_csv(output_dir_test)\n",
    "test_df = pd.read_csv(output_dir_test, header=0, names=['overall', 'reviewText'])\n",
    "\n",
    "print(test_df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15c6a179-81a3-4625-b50f-f4950eb9ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['overall'] = (train_df['overall'] == 2).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7a939838-69b1-41f9-a67d-f8bae92cfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['overall'] = (test_df['overall'] == 2).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f825a9fc-671d-443b-b2bd-5d95822625d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall                                         reviewText\n",
      "0        1  It is a beautiful top! Very soft and very comf...\n",
      "1        0  It's much lighter than the other New Balance r...\n",
      "2        0  I can only use this purse when I have very lit...\n",
      "3        0  I like the style and comfort of the watch.  Th...\n",
      "4        0  Lost my Driver License when the back pocket un...\n",
      "   overall                                         reviewText\n",
      "0        1  My son was so happy with these shoes. I only g...\n",
      "1        0  I bought a large and it does not quite do the ...\n",
      "2        0  Color is not as advertised. It's way darker in...\n",
      "3        0        Its definitely a tote sized bag not a purse\n",
      "4        0  I'm 5'10 140 lbs. I am no means a medium or a ...\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c3be298f-6035-4ee4-852a-d07e11ff1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9d17db46-3b69-434f-97e4-7d0ec8f06e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyarrow version: 12.0.0\n",
      "Datasets version: 2.14.4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pyarrow\n",
    "    import datasets\n",
    "\n",
    "    print(\"Pyarrow version:\", pyarrow.__version__)\n",
    "    print(\"Datasets version:\", datasets.__version__)\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing libraries: {e}\")\n",
    "    # Intentar instalar las bibliotecas necesarias\n",
    "    import os\n",
    "    os.system('pip install pyarrow==12.0.0 datasets==2.14.4')\n",
    "    import pyarrow\n",
    "    import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bd40942c-b217-4813-9e6b-beebb2896bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "08637eb6-bafd-407b-a099-af154205df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 0, 0, 0, 0], 'text': ['It is a beautiful top! Very soft and very comfortable. The only problem is it is a bit big in the bust area for me so I should have ordered a size down.', \"It's much lighter than the other New Balance running shoes, but I like the weight.  It could have been a bit longer, as my longest toe nearly touches the end.  The width (across my toes) is very snug, and could have been  a little wider.\", 'I can only use this purse when I have very little in my purse as the clasp does not like to stay closed.', 'I like the style and comfort of the watch.  The time is always off and the date is never right.  Time seems to get about 3 - 5 mins off every couple of days.  The date is always off.  I have to manually change the date every day.', 'Lost my Driver License when the back pocket unglued by itself.\\n\\nI wish they have used a better glue! :(\\n\\nNeed a new DL now! :X']}\n",
      "Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 158400\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "print(train_dataset[:5])\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9d59e93b-74d5-4d3e-8c14-3c0873d09503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|▏                                                              | 2000/633600 [00:02<10:38, 989.30 examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mremove_columns([col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m test_dataset\u001b[38;5;241m.\u001b[39mcolumn_names \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Tokenizar el dataset de entrenamiento\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Tokenizar el dataset de prueba\u001b[39;00m\n\u001b[0;32m     23\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3091\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   3092\u001b[0m         disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3093\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3094\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3095\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3096\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3097\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3098\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3099\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\datasets\\arrow_dataset.py:3474\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3470\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3471\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3472\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3474\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3478\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3479\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3483\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\datasets\\arrow_dataset.py:3353\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3352\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3353\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3355\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3356\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3357\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[153], line 10\u001b[0m, in \u001b[0;36mtokenize_function\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m      9\u001b[0m     texts \u001b[38;5;241m=\u001b[39m examples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2520\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2519\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2520\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2606\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2601\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2602\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2603\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2604\u001b[0m         )\n\u001b[0;32m   2605\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2608\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2623\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2625\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2627\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2628\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2644\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2645\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2797\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2787\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2788\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2789\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2790\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2794\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2795\u001b[0m )\n\u001b[1;32m-> 2797\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2799\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2815\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\transformers\\tokenization_utils.py:733\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 733\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    735\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\bert-env\\lib\\site-packages\\transformers\\tokenization_utils.py:713\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    714\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    715\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Cargar el tokenizador\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Función de tokenización\n",
    "def tokenize_function(examples):\n",
    "    texts = examples['text']\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "\n",
    "\n",
    "# Eliminar los índices adjuntos si existen\n",
    "train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if 'index' in col])\n",
    "test_dataset = test_dataset.remove_columns([col for col in test_dataset.column_names if 'index' in col])\n",
    "\n",
    "\n",
    "# Tokenizar el dataset de entrenamiento\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Tokenizar el dataset de prueba\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4354aa-8cf0-4454-ba04-d0d2e36b39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la función de tokenización a los datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0977d35-f763-42d1-b0d7-9942d3d63687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d31b4-3da6-4df1-b11b-b3c71915ae02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf3ec8-1f4e-4106-9832-ea539e4b6191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0557c-95c1-4322-82a7-7eedbe6aac60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4da17-7f15-414d-8f93-4a7e804c8f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "058636fb-b230-408c-8486-0022af2c7b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'a', 'sample', 'text', 'for', 'token', '##ization', '.'], ['how', 'are', 'you', 'doing', '?']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = ['This is a sample text for tokenization.', 'How are you doing?']\n",
    "\n",
    "# Tokenizar los textos\n",
    "tokens = [tokenizer.tokenize(text) for text in inputs]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "84068b83-02f0-4e86-84ed-e4112e226780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.DataFrame({\n",
    "    'label': [0, 1, 1, 0],\n",
    "    'text': ['Texto de ejemplo 1', 'Otro texto para entrenar', 'Un tercer texto', 'Último texto']\n",
    "})\n",
    "\n",
    "df_test = pd.DataFrame({\n",
    "    'label': [1, 0, 1],\n",
    "    'text': ['Texto de prueba 1', 'Otro texto de prueba', 'Último texto de prueba']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872ca44-f868-48de-b39a-8c4b71973bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los DataFrames a datasets de Hugging Face\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5cea1329-a183-4a2f-904e-4ccc55353063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Sample:\n",
      "{'label': [1, 0, 0, 0, 0], 'text': ['It is a beautiful top! Very soft and very comfortable. The only problem is it is a bit big in the bust area for me so I should have ordered a size down.', \"It's much lighter than the other New Balance running shoes, but I like the weight.  It could have been a bit longer, as my longest toe nearly touches the end.  The width (across my toes) is very snug, and could have been  a little wider.\", 'I can only use this purse when I have very little in my purse as the clasp does not like to stay closed.', 'I like the style and comfort of the watch.  The time is always off and the date is never right.  Time seems to get about 3 - 5 mins off every couple of days.  The date is always off.  I have to manually change the date every day.', 'Lost my Driver License when the back pocket unglued by itself.\\n\\nI wish they have used a better glue! :(\\n\\nNeed a new DL now! :X']}\n",
      "Test Dataset Sample:\n",
      "{'label': [1, 0, 0, 0, 0], 'text': ['My son was so happy with these shoes. I only gave it 4 stars becuase the dye from his jeans wore off on the \"plastic\" things on the sides of the shoes (triangle things that are attached to the red shoe string threader).  We tried everything to get it to come off, but nothing worked.  He was disappointed in that part, but loved the shoes anyway.', 'I bought a large and it does not quite do the job,a little shorter than I expected.', \"Color is not as advertised. It's way darker in person and definitely not the color (or close to the color) worn on the show.\", 'Its definitely a tote sized bag not a purse', \"I'm 5'10 140 lbs. I am no means a medium or a large but I went for the medium/large based off product reviews. Everyone was right. The costume was too small. I wouldn't recommend this product if you even remotely think it won't fit. Also I only got refunded about 70% of what I paid for it.\"]}\n"
     ]
    }
   ],
   "source": [
    "# Mostrar algunas filas para verificar el formato\n",
    "print(\"Train Dataset Sample:\")\n",
    "print(train_dataset[:5])\n",
    "print(\"Test Dataset Sample:\")\n",
    "print(test_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1723ebf-1893-4fef-a40e-23fe63063e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d3071-476e-4949-97fd-651c8cd4911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento y evaluación del modelo\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a68c44-f572-4155-9d2b-5976e02121bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b48ee1-0f01-4142-85e1-bcc893b6e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación\n",
    "results = trainer.evaluate()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c5175-07e0-4326-9052-81b888f2deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "model.save_pretrained(\"./sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./sentiment_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8967618-fa5c-44ba-8e09-9e2a6796745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de clasificación de una nueva reseña\n",
    "example_review = \"This is the best product I have ever used!\"\n",
    "prediction = classifier(example_review)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062f1ee-2558-428e-91f9-ae80f59d26ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889518e-a01f-47a2-9ff1-694d456f7abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b7aca-65f4-4e2e-8f6b-e5cd096ef930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8c7b98-3625-475b-b990-30bf05a36c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c64c69-eea4-4566-9e24-e732e217df3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca21e0-7412-4832-ac3f-ed0480d0b638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d7d28-19e0-481e-9355-428767e38cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932bbb1-2ab1-47ee-a923-82d7c49d9a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208841bd-ca25-4278-939d-db444a683ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a737559-cd11-42f2-88e9-3f8f354b68a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ae8ff-8d4f-40cd-a1ff-f30d44645455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fbc94-e837-4a33-9477-bed671bf0e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59faea79-a0f9-4162-9266-81ece32ffb33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab74ab1-fac4-4925-afbf-a3ecf62ae32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ecb0c-c468-425c-bbb6-99db1a89ec93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9708b-ecbd-4daf-b35b-25fc10680648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5d2a5-9e8c-4094-99ae-240295a47ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
