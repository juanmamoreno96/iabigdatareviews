{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd82CFhYntkf"
      },
      "source": [
        "# Detección de reseñas positivas y negativas de una tienda de ropa usando XLM-RoBERTa (base-sized model)\n",
        "\n",
        "\n",
        "En este proyecto, vamos a aplicar técnicas avanzadas de procesamiento del lenguaje natural (NLP) utilizando BERT (Bidirectional Encoder Representations from Transformers) para analizar y clasificar reseñas de clientes de una tienda de ropa como positivas o negativas. Utilizaremos un conjunto de datos que contiene reseñas etiquetadas previamente para entrenar y evaluar nuestro modelo.\n",
        "\n",
        "### RoBERTa\n",
        "El transformador RoBERTa es un modelo de lenguaje natural pre-entrenado basado en la arquitectura de los transformers. Fue desarrollado por Facebook AI Research (FAIR) y es una versión optimizada del modelo BERT de Google, con una arquitectura más grande y mejoras en el proceso de entrenamiento.\n",
        "\n",
        "RoBERTa posee un enfoque de pre-entrenamiento no supervisado para el aprendizaje de representaciones de alta calidad del lenguaje natural. Esto se consigue mediante la tarea de \"llenado de huecos\" en el corpus de entrenamiento, donde el modelo debe predecir la palabra restante en una oración determinada.\n",
        "\n",
        "Tras el pre-entreno del modelo, se puede hacer uso para una variedad de tareas de procesamiento de lenguaje natural, como la clasificación de sentimientos, la generación de texto y la traducción de idiomas. El rendimiento que ha demostrado RoBERTa ha sido muy bueno en muchas de estas tareas y es uno de los modelos de lenguaje natural pre-entrenados más usados hoy en día.\n",
        "\n",
        "\n",
        "### Problema a resolver\n",
        "- **PROBLEMA**: _Debido a las altas devoluciones que se están produciendo en una tienda de ropa local, se necesita reducir el número de éstas para poder yo como dueño de la tienda ver cuales son las causas por los que el usuario devuelve la prenda. Así poder mejorar las caracteristicas de las prendas o qué tipo de ropa quieren los compradores que tenga en dicha tienda_\n",
        "\n",
        "\n",
        "- **SOLUCIÓN**: La solución a lo anterior se basa en clasificar una opinión de una prenda en POSITIVA o NEGATIVA mediante el uso de técnicas de procesamiento de lenguaje natural y Deep Learning.\n",
        "\n",
        "### Técnicas y métodos para la resolución del problema\n",
        "\n",
        "- Uso de notebook en Google Colab para la utilización de su GPU.\n",
        "\n",
        "- Uso de la librería transformers de [Hugging Face](https://huggingface.co/), eligiendo el modelo [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) para realizar el procesamiento del texto y agregando dos capas neuronales para mejorar el rendimiento.\n",
        "\n",
        "El modelo fue entrenado con lenguaje español utilizando el dataset de [Kaggle](https://www.kaggle.com/datasets/shavilyarajput/clothing-shoes-and-jewellery-reviews), que contiene varias reviews de productos de una tienda de ropa.\n",
        "\n",
        "- Se utilizó la métrica de evaluación del modelo: Accuracy,  debido a que el equilibrio de la distribución de los ejemplos era bueno.\n",
        "\n",
        "\n",
        "# Despliegue del modelo\n",
        "Se ha elaborado una interfaz gráfica del modelo que se puede apreciar en el siguiente enlace --> https://reviewsiabigdata.streamlit.app/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2xbQpcoKRr6"
      },
      "source": [
        "# 1. Instalación de librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vbubgPZnoBz",
        "outputId": "56e1151e-5369-440e-f10b-d0bd764ef9e0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Collecting easynmt\n",
            "  Downloading EasyNMT-2.0.2.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from easynmt) (2.3.0+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from easynmt) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from easynmt) (0.1.99)\n",
            "Collecting fasttext (from easynmt)\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from easynmt) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->easynmt)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->easynmt)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.2 (from fasttext->easynmt)\n",
            "  Using cached pybind11-2.13.1-py3-none-any.whl (238 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext->easynmt) (67.7.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->easynmt) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->easynmt) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->easynmt) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->easynmt) (1.3.0)\n",
            "Building wheels for collected packages: easynmt, fasttext\n",
            "  Building wheel for easynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easynmt: filename=EasyNMT-2.0.2-py3-none-any.whl size=19902 sha256=7c0aa74a1e273258fe1b2206f81e36c6e2b2ba9c7200960c3f32d47edb5fdf29\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/55/72/aba4face7eac1d7750ca700aa1797b135fb8915e949da504cc\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4246770 sha256=ca934bd41221a1f0996c36e8b1bf9512c36fad531a7a21cc19afc3434f3df0d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built easynmt fasttext\n",
            "Installing collected packages: pybind11, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext, nvidia-cusolver-cu12, easynmt\n",
            "Successfully installed easynmt-2.0.2 fasttext-0.9.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pybind11-2.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets easynmt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlw65uieKZVu"
      },
      "source": [
        "# 2. Importación de librerias y modulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mc8cWZgOo-fw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split  # Para dividir los datos en conjuntos de entrenamiento y prueba\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
        "# Importaciones relacionadas con Transformers:\n",
        "# - AdamW: Optimizador específico para modelos de Transformers que implementa la variante de Adam con corrección de pesos.\n",
        "# - get_linear_schedule_with_warmup: Ayuda a ajustar la tasa de aprendizaje durante el entrenamiento.\n",
        "# - AutoTokenizer: Permite cargar un tokenizador específico del modelo automáticamente.\n",
        "# - AutoModelForMaskedLM: Modelo preentrenado para tareas de Masked Language Modeling (MLM).\n",
        "# - AutoModel: Permite cargar un modelo preentrenado automáticamente según el nombre proporcionado.\n",
        "\n",
        "import torch  # Biblioteca principal de PyTorch para aprendizaje profundo.\n",
        "from torch import nn, optim  # Submódulos de PyTorch para redes neuronales y optimización.\n",
        "from torch.utils.data import Dataset, DataLoader  # Funcionalidades para manejo de datos y carga eficiente.\n",
        "import pandas as pd  # Librería para manipulación y análisis de datos tabulares.\n",
        "from textwrap import wrap  # Utilidad para envolver texto en líneas de ancho fijo.\n",
        "import math  # Funciones matemáticas estándar.\n",
        "import numpy as np  # Biblioteca para soporte de matrices y operaciones numéricas eficientes.\n",
        "import torch.nn.functional as F  # Funciones de activación y capas de PyTorch.\n",
        "from easynmt import EasyNMT  # Biblioteca para fácil implementación de modelos de traducción automática.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Debemos subir a Google el archivo: balanced_review.csv para proceder a leerlo posteriormente\n",
        "df = pd.read_csv('balanced_review.csv', delimiter=\",\", quotechar='\"')"
      ],
      "metadata": {
        "id": "KWFnwYmhZB8N"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total rows: {len(df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vae5dItJXHK",
        "outputId": "37ceefc4-98fe-498c-a3da-bb5cd90d935c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 792000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SzvrFD3Kd2O"
      },
      "source": [
        "# 3. Procesamiento de dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YBx1vdgTGV4",
        "outputId": "6d383a0a-a649-4617-9532-cea7d4daad87",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Se inicializa\n",
        "np.random.seed(42) # Elijo semilla\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Activo GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el dataset\n",
        "file_path = 'balanced_review.csv'\n",
        "df = pd.read_csv('balanced_review.csv', delimiter=\",\", quotechar='\"')\n",
        "print(df.head())\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "\n",
        "# Dividir el dataset en entrenamiento y resto (validación + prueba)\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Dividir el resto en validación y prueba\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Train set: {len(train_df)} rows\")\n",
        "print(f\"Validation set: {len(val_df)} rows\")\n",
        "print(f\"Test set: {len(test_df)} rows\")\n",
        "\n",
        "# Guardar los subconjuntos en archivos CSV\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "val_df.to_csv('validation.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)\n",
        "\n",
        "print(\"Datasets guardados en archivos CSV.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fewDXGUQdUEG",
        "outputId": "a90a6f56-a01b-449c-8d5c-01d7b8302900"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   overall                                         reviewText  \\\n",
            "0        5  I'm enjoying these, the fit is about right and...   \n",
            "1        5  these are the coolest little board shorts.  Th...   \n",
            "2        5  beautiful ring for the money, looks very expen...   \n",
            "3        5  I love this dress! Fantastic quality, great st...   \n",
            "4        5  These Ecco golf shoes are differently shaped t...   \n",
            "\n",
            "              summary  \n",
            "0    Very comfortable  \n",
            "1   funky board short  \n",
            "2          Five Stars  \n",
            "3   I love this dress  \n",
            "4  Still A Great Fit!  \n",
            "Total rows: 792000\n",
            "Train set: 554400 rows\n",
            "Validation set: 118800 rows\n",
            "Test set: 118800 rows\n",
            "Datasets guardados en archivos CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los archivos CSV en DataFrames de pandas\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_validation = pd.read_csv('validation.csv')\n",
        "df_test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "4UYMalJqHjcA"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtro para no tener ejemplos de 3 calificaciones ya que las voy a considerar neutros\n",
        "df_train = df_train[df_train['overall'] != 3]"
      ],
      "metadata": {
        "id": "qjyRyagVHjpo"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorizo en una variable\n",
        "# 1 o 2 calificaciones = 0\n",
        "# 4 o 5 calificaciones = 1\n",
        "def merge_star_ratings(row):\n",
        "    if row['overall'] <= 2:\n",
        "        label = 0\n",
        "    else:\n",
        "        label = 1\n",
        "    return label\n",
        "\n",
        "# Aplico la función de categorización a cada fila del DataFrame\n",
        "df_train['labels'] = df_train.apply(merge_star_ratings, axis=1)\n",
        "df_validation['labels'] = df_validation.apply(merge_star_ratings, axis=1)\n",
        "df_test['labels'] = df_test.apply(merge_star_ratings, axis=1)\n",
        "\n",
        "# Muestra de 5000 ejemplos para el conjunto de entrenamiento\n",
        "df_train = df_train.sample(n=5000, random_state=42)\n",
        "\n",
        "# Utilizamos df_train, df_validation y df_test como nuestro conjunto de datos\n",
        "tienda_train = df_train.copy()\n",
        "tienda_val = df_validation.copy()\n",
        "tienda_test = df_test.copy()\n"
      ],
      "metadata": {
        "id": "4RNAt_paIJHB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizamos el tamaño de cada muestra\n",
        "print(f\"Train: {tienda_train.shape[0]}, Val: {tienda_val.shape[0]}, Test: {tienda_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD9q46AMILbU",
        "outputId": "54c28f0f-f8ca-4ed4-9662-e12723093a4d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 5000, Val: 118800, Test: 118800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizamos que la distribucion es equilibrada\n",
        "tienda_train.labels.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks46Ay5wIM2X",
        "outputId": "b598c88b-bfa2-468d-f723-7480631aaecf"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "labels\n",
              "0    2544\n",
              "1    2456\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubE4fDnQeIrv",
        "outputId": "3ccd1271-908d-49de-f368-3d61dbf2aa92",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 5000, Val: 118800, Test: 118800\n"
          ]
        }
      ],
      "source": [
        "# Tamaños de las muestras\n",
        "print(f\"Train: {tienda_train.shape[0]}, Val: {tienda_val.shape[0]}, Test: {tienda_test.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh0VZgp3LjYo",
        "outputId": "fb7e4bca-d1bb-4c37-a4a7-b1f0e9b82a5c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "labels\n",
              "0    2544\n",
              "1    2456\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "# Observo que la distribucion es equilibrada\n",
        "tienda_train.labels.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkOvXG94oZSj"
      },
      "source": [
        "# 4. Tokenización del texto  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "yEsKKezJVZEW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Se carga el modelo para tokenizar el texto\n",
        "\n",
        "model_name = 'xlm-roberta-base'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ8Wcorxo5sL"
      },
      "source": [
        "# 5. Dataset y DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "pqv-DZtzYQhm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Creamos nuestra clase Dataset\n",
        "\n",
        "class TiendaDataset(Dataset):\n",
        "\n",
        "  def __init__(self,reviews,labels,tokenizer,max_len : int= 250):\n",
        "    self.reviews = reviews # Texto\n",
        "    self.labels = labels # Etiquetas\n",
        "    self.tokenizer = tokenizer # Tokenizer\n",
        "    self.max_len = max_len #maximo de longitud de texto 250 caracteres\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.reviews)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item]) # Iterador\n",
        "    label = self.labels[item]\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        review,\n",
        "        max_length = self.max_len,\n",
        "        truncation = True,\n",
        "        add_special_tokens = True,\n",
        "        return_token_type_ids = False,\n",
        "        padding = \"max_length\",\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt'\n",
        "        )\n",
        "\n",
        "\n",
        "    return {\n",
        "          'review': review,\n",
        "          'input_ids': encoding['input_ids'].flatten(),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'label': torch.tensor(label, dtype=torch.long)\n",
        "      }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "NaYe0FsoaNAv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Creo funcion Dataloader\n",
        "\n",
        "def Tienda_data_loader(df, tokenizer, max_len : int= 250, batch_size : int= 16):\n",
        "  dataset = TiendaDataset(\n",
        "      reviews = df.reviewText.to_numpy(),\n",
        "      labels = df.labels.to_numpy(),\n",
        "      tokenizer = tokenizer\n",
        "  )\n",
        "\n",
        "  return DataLoader(dataset, batch_size, num_workers = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9rNI4Tsa5MZ",
        "outputId": "c19cca9c-6a50-466e-85f2-12c30d5f8add",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Instancio los DataLoaders\n",
        "\n",
        "train_data_loader = Tienda_data_loader(tienda_train, tokenizer)\n",
        "val_data_loader = Tienda_data_loader(tienda_val, tokenizer)\n",
        "test_data_loader = Tienda_data_loader(tienda_test, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q53-bDpjpBX8"
      },
      "source": [
        "# 6. Modelo - RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "VOnca-sVbhPX",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Se crea el modelo\n",
        "\n",
        "class RobertaModel(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes: int = 2):\n",
        "    super().__init__()\n",
        "    self.roberta = AutoModel.from_pretrained(\"xlm-roberta-base\") # Transformer\n",
        "    self.dropout = nn.Dropout(p=0.3) #Añadimos un Dropout para disminuir el overfitting\n",
        "    self.linear = nn.Linear(self.roberta.config.hidden_size, self.roberta.config.hidden_size) # 1er capa linear\n",
        "    self.classification = nn.Linear(self.roberta.config.hidden_size, n_classes) # 2da capa linear\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    #Roberta layer\n",
        "    cls_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    pooled_output = torch.mean(cls_output.last_hidden_state, 1)\n",
        "\n",
        "\n",
        "    # Red Nueronal\n",
        "    pooled_output = self.linear(pooled_output) # Primera capa\n",
        "    pooled_output = F.relu(pooled_output) # Funcion de activacion relu\n",
        "    pooled_output = self.dropout(pooled_output) # Dropout\n",
        "    output = self.classification(pooled_output) #Segunda capa\n",
        "\n",
        "    return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "CanuNncKdXaF",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Instancio modelo\n",
        "model = RobertaModel().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRjTm2SEpWWj"
      },
      "source": [
        "# 7. Optimizacion modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Mansb6pfdn_H",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Optimizacion y parametros de entrenamiento\n",
        "EPOCHS = 5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.001)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = math.floor(total_steps * 0.2),\n",
        "    num_training_steps = total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJATL2gRpbm_"
      },
      "source": [
        "# 8. Loops de Train y Evaluacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ihoNOG0Zeuhz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Defino funcion de loop de Train\n",
        "\n",
        "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for batch in data_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "    outputs = model(input_ids, attention_mask)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double()/n_examples, np.mean(losses)\n",
        "\n",
        "# Defino funcion de loop de Evaluacion\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "      outputs = model(input_ids, attention_mask)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double()/n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLm_MSv4ppNQ"
      },
      "source": [
        "# 9. Train y Validacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMhFkVSbZ-JZ",
        "outputId": "839eda90-4dc9-4d2e-c480-ff158460549c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpochs:   0%|          | 0/5 [00:00<?, ?epoch/s]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "results = {\"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": [] }\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS), desc='Epochs', unit='epoch'):\n",
        "    train_acc, train_loss = train_model(\n",
        "        model, train_data_loader, loss_fn, optimizer, device, scheduler, len(tienda_train)\n",
        "    )\n",
        "    val_acc, val_loss = eval_model(\n",
        "        model, val_data_loader, loss_fn, device, len(tienda_val)\n",
        "    )\n",
        "    print('Entrenamiento: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n",
        "    print('Validación: Loss: {}, accuracy: {}'.format(val_loss, val_acc))\n",
        "    print('')\n",
        "\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc.cpu())\n",
        "    results[\"val_loss\"].append(val_loss)\n",
        "    results[\"val_acc\"].append(val_acc.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crcRMvJRp8cH"
      },
      "source": [
        "Podemos observar un valor de Accuracy mayor al 90% en la fase de Train y Validación, con un valor de Loss muy bajo.\n",
        "Por tanto, existe un gran rendiemiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDbCvTHopvMZ"
      },
      "source": [
        "# 10. Gráfico de Loss y Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nVQfgboQ7Hgh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Creo funciones\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(train_loss, val_loss):\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.set_title(\"Loss - Train vs Val\")\n",
        "    ax.plot(train_loss, label=\"Loss Train\")\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(val_loss, label=\"Loss Val\", color=\"orange\")\n",
        "    ax.legend(loc=\"upper left\")\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "    ax.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy(train_acc, val_acc):\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.set_title(\"Accuracy - Train vs Val\")\n",
        "    ax.plot(train_acc, label=\"Accuracy Train\")\n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.plot(val_acc, label=\"Accuracy Val\", color=\"orange\")\n",
        "    ax.legend(loc=\"upper left\")\n",
        "    ax2.legend(loc=\"upper right\")\n",
        "    ax.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "M080kUlm7bFv",
        "outputId": "3312a512-4846-45f9-d214-b31e61cc9ded",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6398884e9632>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ],
      "source": [
        "plot_loss(results[\"train_loss\"], results[\"val_loss\"])\n",
        "plot_accuracy(results[\"train_acc\"], results[\"val_acc\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTYrfaWEp7Wu"
      },
      "source": [
        "**Loss:** En la época 2, se observa la menor pérdida en el conjunto de validación, después de lo cual comienza a aumentar, aunque siempre se mantiene en un nivel muy bajo.\n",
        "\n",
        "**Accuracy:** Se nota un aumento en la exactitud para ambos conjuntos (entrenamiento y validación), alcanzando valores superiores al 90%. No se detecta sobreajuste."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOte9YvUq4ol"
      },
      "source": [
        "# 11. Evaluacion de Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "EY-8PIEdgoKv",
        "outputId": "6560b3be-e227-44f6-8599-6837ce5e8af4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'eval_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-39414e48d6e6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_acc, test_loss = eval_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     model, test_data_loader, loss_fn, device, len(tienda_test))\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test: Loss: {}, accuracy: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_model' is not defined"
          ]
        }
      ],
      "source": [
        "test_acc, test_loss = eval_model(\n",
        "    model, test_data_loader, loss_fn, device, len(tienda_test))\n",
        "\n",
        "print('Test: Loss: {}, accuracy: {}'.format(test_loss, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISRW2Gfeq8rt"
      },
      "source": [
        "Se observa un gran rendimiento (Accuracy mayor a 90%) en el conjunto de Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3z4K0TYrGR_"
      },
      "source": [
        "# 12. Guardar el modelo\n",
        "\n",
        "Se guarda en el inicio de Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "VOS70zpXiFPo",
        "outputId": "98586d20-0ac2-4821-b93d-d024d89b8a29",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-1cff123bc783>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# Dar permisos para guardar en el Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lONlxUbuiVe_",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "072089e4-55d3-440d-a064-c3d5583a4133"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory /content/drive/My Drive does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-77a61ac7a904>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/Modelo_Amazon_review.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/drive/My Drive does not exist."
          ]
        }
      ],
      "source": [
        "# Guardo el modelo\n",
        "\n",
        "PATH = \"/content/drive/My Drive/Modelo_Tienda_review.pt\"\n",
        "\n",
        "torch.save(model.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRhNtF_trWmL"
      },
      "source": [
        "# 13. Evaluacion con texto propio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QTG7YKnVjDA-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Creamos una función personalizada para una determinada reseña y detectar si es positiva o negativa\n",
        "\n",
        "def ClasificacionSentimiento(review_text):\n",
        "    encoding_review = tokenizer.encode_plus(\n",
        "        review_text,\n",
        "        max_length = 250,\n",
        "        truncation = True,\n",
        "        add_special_tokens = True,\n",
        "        return_token_type_ids = False,\n",
        "        padding = \"max_length\",\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt'\n",
        "        )\n",
        "\n",
        "    input_ids = encoding_review['input_ids'].to(device)\n",
        "    attention_mask = encoding_review['attention_mask'].to(device)\n",
        "    output = model(input_ids, attention_mask)\n",
        "    _, prediction = torch.max(output, dim=1)\n",
        "    print(\"\\n\".join(wrap(review_text)))\n",
        "    print(prediction)\n",
        "    if prediction == 0:\n",
        "        print('La reseña es negativa')\n",
        "    else:\n",
        "        print('La reseña es positiva')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "oe0im1_alf4K",
        "outputId": "3110be9d-dc0f-40fd-934b-e30bce7b4401",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-be25fe0755f4>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Para el precio que costaba, la camiseta no era muy bonita\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mClasificacionSentimiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-be5a868c0fc0>\u001b[0m in \u001b[0;36mClasificacionSentimiento\u001b[0;34m(review_text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mClasificacionSentimiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     encoding_review = tokenizer.encode_plus(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mreview_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Llamamos a la función de clasificación\n",
        "\n",
        "text = \"Para el precio que costaba, la camiseta no era muy bonita\"\n",
        "ClasificacionSentimiento(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNlHECVBuDXJ"
      },
      "source": [
        "# 14. Conclusión\n",
        "\n",
        "El modelo ha demostrado un rendimiento bastante bueno en los tres conjuntos de datos del dataset, alcanzando una precisión (Accuracy) superior al 90%.\n",
        "\n",
        "\n",
        "Se desarrolló una interfaz gráfica para facilitar el uso del modelo: https://reviewsiabigdata.streamlit.app/\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}